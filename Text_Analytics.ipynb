{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasansin/Text-Analytics/blob/master/Text_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZuqEy-2y1bO"
      },
      "source": [
        "### CMM706 â€“ Individual Coursework\n",
        "#### June 2024\n",
        "\n",
        "Name  : D.N.H. Weerasinghe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k80XxMQKMi06"
      },
      "outputs": [],
      "source": [
        "!pip install contractions\n",
        "!pip install tabulate\n",
        "!pip install pyspellchecker\n",
        "!pip install scikit-learn\n",
        "!pip install datasets\n",
        "!pip install imblearn\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrYhAHm_y1bP"
      },
      "source": [
        "### Answer to Task 1\n",
        "\n",
        "Describe the dataset provided with this coursework as you understand it, in an appropriate way to top management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLzNZJFr__I3"
      },
      "source": [
        "**1. Import necessary modules for code.**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfTfw6ZK_a1v"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import contractions\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sentencepiece as spm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix,roc_curve, auc, precision_recall_fscore_support,classification_report\n",
        "from sklearn.utils import shuffle\n",
        "from spellchecker import SpellChecker\n",
        "from sklearn.preprocessing import LabelEncoder,label_binarize\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "ps = PorterStemmer()\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RITUEE0VAV2e"
      },
      "source": [
        "\n",
        "\n",
        "**2. Load the dataset**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oAUjcnnAUj9"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "file_path = '/content/Purchase_Intent.csv'\n",
        "dataSet = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gsYz5Oe9P-3"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "**3. Method implementation for task 01**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAFPQOZRBUlw"
      },
      "source": [
        "Below method is used to get the basic details of the dataset - ex dimensions, data types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xej9w__f_7tV"
      },
      "outputs": [],
      "source": [
        "def get_basic_details_of_dataset(data_set):\n",
        "\n",
        "    # get the dimensions of the dataset\n",
        "    dimensions = data_set.shape\n",
        "    print(\"\\n\\n Dataset dimensions:\", dimensions)\n",
        "\n",
        "    #preview data in a table - this only shows first 8 rows of the data set\n",
        "    print(\"\\n\\n Preview first 8 rows of the data set in a table:\")\n",
        "    display(pd.DataFrame(data_set.head(8)))\n",
        "\n",
        "    #preview data in a table - this only shows last 8 rows of the data set\n",
        "    print(\"\\n\\n Preview last 8 rows of the data set in a table:\")\n",
        "    display(pd.DataFrame(data_set.tail(8)))\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = data_set.isnull().sum()\n",
        "\n",
        "    # Get data types and summary statistics\n",
        "    data_types = data_set.dtypes\n",
        "    summary_statistics = data_set.describe(include='all')\n",
        "\n",
        "    # Print the results\n",
        "    print(\"\\n\\n Data types:\")\n",
        "    #print data types of the data set\n",
        "    data_types_df = pd.DataFrame({'Data Types':data_types})\n",
        "    display(data_types_df)\n",
        "\n",
        "    #print the summary statistics of thedata set\n",
        "    print(\"\\n\\n Summary statistics:\")\n",
        "    display(pd.DataFrame(summary_statistics))\n",
        "\n",
        "    #print missing values of the data set\n",
        "    print(\"\\n\\n Missing values:\")\n",
        "    missing_valies_df = pd.DataFrame({'Missing Values':missing_values})\n",
        "    display(missing_valies_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLaSfro4A4-r"
      },
      "source": [
        "*Below method is used to get the duplication tweet count of each class in the dataset*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcO7dNye9Dkg"
      },
      "outputs": [],
      "source": [
        "# Check for duplicate tweets\n",
        "def display_duplicated_tweets():\n",
        "    duplicated_tweet_List = []\n",
        "\n",
        "    #iterrate through data set and create a list with duplication of each class\n",
        "    for name_of_class in dataSet['class'].unique():\n",
        "      duplicate_tweets = dataSet[dataSet['class']== name_of_class].duplicated(subset='text').sum()\n",
        "      duplicated_tweet_List.append({'class name': name_of_class, 'count':duplicate_tweets})\n",
        "\n",
        "    #Display duplicated data in a bar graph\n",
        "    display(pd.DataFrame(duplicated_tweet_List).plot(kind='bar', x='class name', y='count'))\n",
        "\n",
        "    #Display duplicated data in a table\n",
        "    print(\"\\n\\n Duplicate tweets:\")\n",
        "    display(pd.DataFrame(duplicated_tweet_List))\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Z5FFK4AsIS"
      },
      "source": [
        "*Below method is used to get the distribution of the classes in the data set*`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxWXOAP64I-C"
      },
      "outputs": [],
      "source": [
        "# Distribution of the class variable\n",
        "def display_class_distribution(data_set):\n",
        "\n",
        "  # get the data count of each class\n",
        "  class_counts = data_set['class'].value_counts()\n",
        "  print(\"\\n\\nClass counts - Table View:\")\n",
        "\n",
        "  #display class counts in a table\n",
        "  display(pd.DataFrame({'class counts':class_counts}))\n",
        "\n",
        "  # display the distribution of the class variable in a bar graph\n",
        "  print(\"\\n\\n Class counts - Bar graph View:\")\n",
        "  display(pd.DataFrame(data_set['class'].value_counts()).plot(kind='bar'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyieGBoJBuDv"
      },
      "source": [
        "**4. Method execution**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMLCpj-jM9jK"
      },
      "source": [
        "*get basic details of data set*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ir0HOBf8Mvhe"
      },
      "outputs": [],
      "source": [
        "get_basic_details_of_dataset(dataSet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGxXf5CsNQFT"
      },
      "source": [
        "*get duplicated data in each class of data set*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHKPB7BVM0MT"
      },
      "outputs": [],
      "source": [
        "display_duplicated_tweets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJNClkl9NqqX"
      },
      "source": [
        "*display the data distribution of each class before preprocessing data*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfnrrYa-B6oH"
      },
      "outputs": [],
      "source": [
        "display_class_distribution(dataSet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wK6vcMGy1bQ"
      },
      "source": [
        "Explain the challenges encountered and the observations based on the output above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCOeq-2nPICy"
      },
      "source": [
        "\n",
        "### Challenges\n",
        "\n",
        "---\n",
        "\n",
        "*   **Imbalanced Data** - From all  categories in the data set, most occurences belong to the undefined category. 947(both undefiend & Undefiend) out of 2108 belong to the undefined category. This might affect the performance of classification models and the accuracy of insights derived from the data.\n",
        "\n",
        "* **Class Duplication** - Also same class has duplicated. ex - there are two sepearted classes for \"yes\" and \"Yes\" even though, meaning of those are same.\n",
        "\n",
        "### Observations\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   **data quality** - The dataset contains noisy data which mean irrelevnt or inappropriate data which lowers the quality of the data set. As a example, the first and the second record of the data set does not seem to be related to a purchasing.\n",
        "\n",
        "* **Imbalanced Data** - Most of data is under the undefined category. This might be due to the misclassifed of missing data, which needs to be corrected to ensure accurate labeling.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiVJi2P7y1bQ"
      },
      "source": [
        "### Answer to Task 2\n",
        "\n",
        "Perform any further cleaning required, tokenize the data and prepare the data for model building. Describe this final dataset in terms of the number of documents (tweets), the number of total words and the number of unique words in the corpus as a whole. Describe also the number of documents in each of the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoJtmmw1EfXV"
      },
      "source": [
        "**1. Implement methods to perform cleaning & preprocesssing**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YBfTkcNGy1bQ"
      },
      "outputs": [],
      "source": [
        "## Preprocessing Methods\n",
        "\n",
        "#expand contractions - expand words like it's to it is\n",
        "def expandContractions(text):\n",
        "  return contractions.fix(text)\n",
        "\n",
        "#correct spellings - corrects the wrong spellings\n",
        "def correct_spellings(text):\n",
        "  words = text.split()\n",
        "  corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in words]\n",
        "  return ' '.join(corrected_words)\n",
        "\n",
        "#convert text to lowercase -covert text in text section and class section to lower case\n",
        "def convertTolowerCase(data_set):\n",
        "  data_set['text'] = data_set['text'].str.lower()\n",
        "  data_set['class'] = data_set['class'].str.lower()\n",
        "\n",
        "#remove mentions in text fields -  remove words next to @\n",
        "def removeMentions(text):\n",
        "  return re.sub(r'@\\S+', '',text)\n",
        "\n",
        "#remove extra spaces may include in text sentences\n",
        "def removeExtraSpaces(text):\n",
        "  return re.sub(r'\\s+', ' ',text).strip()\n",
        "\n",
        "#remove special charcters\n",
        "def removeSpecialCharacters(text):\n",
        "  return re.sub(r'[^\\w\\s]', '',text)\n",
        "\n",
        "#remove numerical values since these are cause noice\n",
        "def removeNumericalValues(text):\n",
        "  return re.sub(r'\\d+', '',text)\n",
        "\n",
        "#remove undefiend class from the dataset\n",
        "def remove_class_undefined(data_set):\n",
        "   data_set.drop(data_set[data_set['class'] == 'undefined'].index, inplace=True)\n",
        "\n",
        "#get all undefined data\n",
        "def get_undefined_data(data_set):\n",
        "  return data_set[data_set['class'] == 'undefined']\n",
        "\n",
        "#remove stop words - removes stop words in english\n",
        "def removeStopWords(text):\n",
        "  words = tokenize(text)\n",
        "  stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "  negations = ['not', 'no',\"nor\", 'never','neither']\n",
        "  stop_words = stop_words.difference(negations)\n",
        "  filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "  return ' '.join(filtered_words)\n",
        "\n",
        "# tokenize text in text section with using nltk tokenizer\n",
        "def tokenize(text):\n",
        "  #tokenize text from text column\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Annotate text tokens with POS tags\n",
        "def pos_tag_text(text):\n",
        "\n",
        "    def penn_to_wn_tags(pos_tag):\n",
        "        if pos_tag.startswith('J'):\n",
        "            return wn.ADJ\n",
        "        elif pos_tag.startswith('V'):\n",
        "            return wn.VERB\n",
        "        elif pos_tag.startswith('N'):\n",
        "            return wn.NOUN\n",
        "        elif pos_tag.startswith('R'):\n",
        "            return wn.ADV\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    tokens = tokenize(text)\n",
        "    tagged_text = nltk.pos_tag(tokens)\n",
        "\n",
        "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
        "                         for word, pos_tag in\n",
        "                         tagged_text]\n",
        "    return tagged_lower_text\n",
        "\n",
        "# lemmatize text based on POS tags\n",
        "def lemmatize_text(text):\n",
        "    pos_tagged_text = pos_tag_text(text)\n",
        "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
        "                         else word\n",
        "                         for word, pos_tag in pos_tagged_text]\n",
        "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "    return lemmatized_text\n",
        "\n",
        "##get th summerize data after preprcessing is done\n",
        "def summerize_data_after_preprocessing(data_set):\n",
        "    print('\\n\\n-------------------------------------------------------> ')\n",
        "    print(\"\\033[1mSummerize Data After Preprocessing\\033[0m\")\n",
        "    print('------------------------------------------------------->')\n",
        "    display_class_distribution(data_set)\n",
        "    get_basic_details_of_dataset(data_set)\n",
        "\n",
        "\n",
        "#data cleaning\n",
        "def dataPreprocessing(data_set,isNotTokenizing) :\n",
        "\n",
        "    #convert the data into lowercase - both class and text data\n",
        "    convertTolowerCase(data_set)\n",
        "\n",
        "    #expand contractions - this expands the words like don't, haven't in to do not, have not\n",
        "    data_set['text'] = data_set['text'].apply(lambda x: expandContractions(x))\n",
        "\n",
        "    #removing extra spaces- removes the extra space findes insides the texts\n",
        "    data_set['text'] = data_set['text'].apply(removeExtraSpaces)\n",
        "\n",
        "    #removing mentions - removes @sign related text\n",
        "    data_set['text'] = data_set['text'].apply(removeMentions)\n",
        "\n",
        "    #removing special charaters - removes special characters to reduce noice\n",
        "    data_set['text'] = data_set['text'].apply(removeSpecialCharacters)\n",
        "\n",
        "    #removing numerical values\n",
        "    data_set['text'] = data_set['text'].apply(removeNumericalValues)\n",
        "\n",
        "    #remove stop words\n",
        "    data_set['text'] = data_set['text'].apply(removeStopWords)\n",
        "\n",
        "    if isNotTokenizing:\n",
        "      #lemmatize text\n",
        "      data_set['text'] = data_set['text'].apply(lemmatize_text)\n",
        "\n",
        "    return data_set\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7JEmYNpFAtl"
      },
      "source": [
        "**2. Creating a Binary Classifier to Calssify Undefined Data**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7PuxNSt-t_p"
      },
      "source": [
        "Below method is used classify undefiened data. In here, when creating the classifier, there was a issue with the data set since the data is still unbalaced.\n",
        "\n",
        " To balance the data SMOTE oversampling is used. This oversampling is only performed for the training dataset in order to avoid the data leakage which means, same data rows get include in both test and train data. Another reason to do this only for training data is, to avoid overfitting by providing a balanced data set for the model during the training & not resampling test data allow model to perform more accurately.\n",
        "\n",
        " Moreover TfidfVectorizer is used in here instead using CounterVerctor. Reason for that is Counterverctor is treats all words equally(inmportance in not considered) and simply count the frequancies of each word while,TfidfVectorizer capures the imporatant terms in the entire corpus.\n",
        "\n",
        " Using this vectoriser we conver text data to numerical values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC_dswCMOl4y"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def classify_undefiened_data(data_set):\n",
        "    #get undefined data\n",
        "    undefiend_data = get_undefined_data(data_set);\n",
        "\n",
        "    #remove undefiend class\n",
        "    remove_class_undefined(data_set)\n",
        "\n",
        "    #splitting data set in to train and test data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data_set['text'], data_set['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "    #vectorize data using tfidf vectorizer\n",
        "    tfidfVectorizer = TfidfVectorizer()\n",
        "\n",
        "    #fit and transform the train data in to a vector\n",
        "    X_train_vector = tfidfVectorizer.fit_transform(X_train)\n",
        "\n",
        "    #transform the test data in to a vector\n",
        "    X_test_vector = tfidfVectorizer.transform(X_test)\n",
        "\n",
        "    print(\"\\nbefore resampling>\", Counter(y_train))\n",
        "\n",
        "    #oversampling the class which has minor values\n",
        "    oversampling_smt= SMOTE(random_state=42)\n",
        "\n",
        "    #resampling the train data -  resampling only train data prevents data leakage\n",
        "    X_train_resampled, y_train_resampled = oversampling_smt.fit_resample(X_train_vector, y_train)\n",
        "    print('\\nafter resampling', Counter(y_train_resampled))\n",
        "\n",
        "    #create and train the model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "    #predict  data using vectozied test data\n",
        "    y_predict = model.predict(X_test_vector)\n",
        "\n",
        "    print(\"\\naccuracy of the model:\",accuracy_score(y_test, y_predict))\n",
        "\n",
        "\n",
        "    #convert undefiend data in to a vector using tfidf vectorizer\n",
        "    undefiend_data_x_vector = tfidfVectorizer.transform(undefiend_data['text'])\n",
        "\n",
        "    #get the probabilities using created model\n",
        "    probabilites = model.predict_proba(undefiend_data_x_vector)\n",
        "\n",
        "    #set confidence threshold\n",
        "    confidence_threshold = 0.7\n",
        "\n",
        "    #get high confidence indices\n",
        "    hgh_cn_ind = (probabilites.max(axis=1) > confidence_threshold)\n",
        "\n",
        "    #get high confidence predictions\n",
        "    hgh_cn_pd = model.classes_[probabilites.argmax(axis=1)[hgh_cn_ind]]\n",
        "\n",
        "    #add classified data in to original data set\n",
        "    classified_text_data = undefiend_data['text'][hgh_cn_ind]\n",
        "\n",
        "    #get the classified data as a data frame\n",
        "    classified_data = pd.DataFrame({'text': classified_text_data, 'class': hgh_cn_pd})\n",
        "\n",
        "    #add classified data in to original data set\n",
        "    combind_data = pd.concat([data_set, classified_data], ignore_index=True)\n",
        "\n",
        "    #shuffle the data set as the final step\n",
        "    combind_data = shuffle(combind_data,random_state=42)\n",
        "\n",
        "    #update the data set\n",
        "    return combind_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6hKlXx5Eyj4"
      },
      "source": [
        "**3. Perform data preprocessing & classify undefined data**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTzcggdAEtQB"
      },
      "outputs": [],
      "source": [
        "#perform data preprocessing\n",
        "processed_data = dataPreprocessing(dataSet,True);\n",
        "\n",
        "#classify undefined data in to yes or no classes\n",
        "classified_data = classify_undefiened_data(processed_data);\n",
        "\n",
        "#summrization of above two processes\n",
        "summerize_data_after_preprocessing(classified_data);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPnYaEEH2ff2"
      },
      "source": [
        "**5. Implement method to display dataset after above steps**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO5xP2nz2evv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def display_preprocessed_dataset(data_set):\n",
        "   #number of documents (tweets)\n",
        "   print(\"\\n\\nNumber of documents (tweets):\", data_set.shape[0])\n",
        "\n",
        "   # the number of total words\n",
        "   total_words = sum(len(word.split()) for word in data_set['text'])\n",
        "   print(\"\\n\\nThe number of total words:\", total_words)\n",
        "\n",
        "   #the number of unique words in the corpus as a whole\n",
        "   unique_words = set()\n",
        "   for text in data_set['text']:\n",
        "       words = text.split()\n",
        "       unique_words.update(words)\n",
        "   print(\"\\n\\nThe number of unique words in the corpus as a whole:\", len(unique_words))\n",
        "\n",
        "\n",
        "   #preview data in a table - this only shows first 8 rows of the data set\n",
        "   print(\"\\n\\nPreview first 8 rows of the data set in a table:\")\n",
        "   display(pd.DataFrame(data_set.head(8)))\n",
        "\n",
        "   #number of documents in each of the classes\n",
        "   class_counts = data_set['class'].value_counts()\n",
        "   class_count_in_DF = pd.DataFrame({'class counts':class_counts})\n",
        "   print(\"\\n\\nPreview Number of documents in each of the classes in table:\")\n",
        "   display(class_count_in_DF)\n",
        "\n",
        "   #preview data in a bar graph\n",
        "   print(\"\\n\\nPreview Number of documents in each of the classes in a bar graph:\")\n",
        "   display(pd.DataFrame(data_set['class'].value_counts()).plot(kind='bar'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgnGPWCtWAbn"
      },
      "source": [
        "**6. Perform display data method**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnnaR-hay1bR"
      },
      "source": [
        "Describe the final dataset shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba-BqsBnc_W1"
      },
      "outputs": [],
      "source": [
        "display_preprocessed_dataset(classified_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPtCTT0G0xst"
      },
      "source": [
        "**In above section. data preprocessing performed and then using preprocessed data create a classifier to undefined data. After that, using the classifier undefined data is get classified and combined to the dataset with the given class labels. Finally,to display the cimbined dataset's specification \"summerize_data_after_preprocessing\"  method is used.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EzxsyeQDQkx"
      },
      "source": [
        "**Since undefined data count is higher, using a binary classiffer,classified the undefiened data in to yes or no classes to increase the performance. most of unwanted data is now removed from the data set,like stop word except negations. Reason for not removing negation is that we need negations to decide the purchasing intent and in a binary classification.It is must to have positive and negative data(If negations are removed then the negative data will become positive).\n",
        "Class duplications removed by lower casing whole dataset in data preprocessing.\n",
        "Now data has only two classes as 'yes' and 'no'. With using undefiend data classifier has increased the data counts of each class. ex- previously \"yes\" & \"Yes\" had only 696. Now \"yes\" class count is 898.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTwDpqXhy1bR"
      },
      "source": [
        "### Answer to Task 3\n",
        "\n",
        "Use a non-deep learning classification algorithm in order to define a baseline for exploring further model building. Report the performance of the baseline model and interpret the model and its predictions to top management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgEUOReUyylk"
      },
      "source": [
        "**1. Implement method for baseline model**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J36OAabJOcan"
      },
      "source": [
        "here also used same resampling technique used in undefiend data classifier. Same vectorizer used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKaYO0k0y1bR"
      },
      "outputs": [],
      "source": [
        "#baseline model - Logistic Regression\n",
        "def create_baseline_model(data_set):\n",
        "    #check if data is set of token list\n",
        "    if isinstance(data_set['text'].iloc[0],list):\n",
        "      data_set['text'] = data_set['text'].apply(lambda text : ''.join(text))\n",
        "\n",
        "    tfidf_Vectorizer = TfidfVectorizer()\n",
        "\n",
        "    #\n",
        "    X_data = tfidf_Vectorizer.fit_transform(data_set['text'])\n",
        "    y_data = data_set['class']\n",
        "\n",
        "    #split data set in to train and test data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
        "    print(\"\\n before resampling>\", Counter(y_train))\n",
        "\n",
        "    #as preprocessed data shown in task 2, class **yes** has more data than the **no** class this might affect  the accuracy of the model. so we need to resample this data set\n",
        "    oversampler = SMOTE(random_state=42)\n",
        "    #to avoid data leakage oversampling is only perform to the training data.\n",
        "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "    print(\"\\n after resampling>\", Counter(y_train_resampled))\n",
        "\n",
        "    #training model with max iteration of 1000\n",
        "    lr_classifier = LogisticRegression(max_iter= 1000)\n",
        "\n",
        "    #fit the train data\n",
        "    lr_classifier.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "    #predict using classifier\n",
        "    y_pred = lr_classifier.predict(X_test)\n",
        "\n",
        "\n",
        "    #performence metrics of baseline model\n",
        "\n",
        "    if data_set['class'].dtype == 'object': #String labels\n",
        "        pos_label = 'no'\n",
        "    else:  # Numerical labels\n",
        "        pos_label = 0\n",
        "\n",
        "    #accuracy of the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    #precision of the model\n",
        "    precision = precision_score(y_test, y_pred,pos_label=pos_label)\n",
        "\n",
        "    #recall of the model\n",
        "    recall = recall_score(y_test, y_pred,pos_label=pos_label)\n",
        "\n",
        "    #f1 score of the model\n",
        "    f1 = f1_score(y_test, y_pred,pos_label=pos_label)\n",
        "\n",
        "    #confusion metrics for baseline model\n",
        "    cmonfusion_metrics= confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    #convert string labels to numeric using LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "    y_test_numeric = le.fit_transform(y_test)\n",
        "\n",
        "\n",
        "    y_test_bin = label_binarize(y_test_numeric, classes=np.unique(y_test_numeric))\n",
        "    y_pred_prob = lr_classifier.predict_proba(X_test)\n",
        "\n",
        "    roc_data = { \"y_test_bin\":y_test_bin, \"y_pred_prob\":y_pred_prob, \"y_test_numeric\":y_test_numeric }\n",
        "\n",
        "    return accuracy, precision, recall, f1, cmonfusion_metrics ,roc_data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc2cCGb5zCIP"
      },
      "source": [
        "**2. Define the baseline by invoking above method**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWx5Q8E54ZJ1"
      },
      "source": [
        "below code perform the baseline model to get the accuracy and  other metrics to understand the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9-dUU3jzPVB"
      },
      "outputs": [],
      "source": [
        "basline_results = create_baseline_model(classified_data);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5zMEMnOy1bS"
      },
      "source": [
        "Report on the baseline model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5Ef1y9RzfV1"
      },
      "source": [
        "**3. Display Accuracy, Precision, Recall & F1-score in a bar chart & in a Table**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8qa7Eb91E6v"
      },
      "outputs": [],
      "source": [
        "#plot baseline model performence using the data recieved from above method invocation\n",
        "accuracy, precision, recall, f1, cmonfusion_metrics ,roc_data = basline_results\n",
        "\n",
        "print('\\n \\n Accuracy, Precision, Recall & F1-score of Baseline Model in a Table \\n\\n')\n",
        "display(pd.DataFrame({'accuracy': [accuracy], 'precision': [precision], 'recall': [recall], 'f1_score': [f1]}))\n",
        "\n",
        "#create a dataframe using data recieves from\n",
        "\n",
        "print('\\n \\n Accuracy, Precision, Recall & F1-score of Baseline Model in a Bar Graph \\n\\n')\n",
        "accuracy_df = pd.DataFrame({'Percentages':[accuracy,precision,recall,f1],\"Accuracy Metrics\":['accuracy', 'precision', 'recall', 'f1_score']}).plot(x='Accuracy Metrics', y='Percentages', kind='bar')\n",
        "\n",
        "#display dataframe\n",
        "display(accuracy_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2TqG9bU6f2-"
      },
      "outputs": [],
      "source": [
        "#plot confusion metrics\n",
        "#this helps to understand the false positive, false negative, true negative and true positive\n",
        "#provides an idea about with which class that model perform well.\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(cmonfusion_metrics, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-QXUza7Jvvo"
      },
      "source": [
        "In above graph true negative has 90 data points and true positive has 159 while false negative has 24 data point and false positive has 18 data points.\n",
        "This has lead to the acuuracy depict in above table.(TP - True Positive , TN - True Negative, FP - False Positive , FN - False Negative\n",
        ")\n",
        "\n",
        "Accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
        "\n",
        "Precision = TP / (TP + FP)\n",
        "\n",
        "\n",
        "Recall = TP / (TP + FN)\n",
        "\n",
        "\n",
        "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrn2ge8xlzC1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert string labels to numeric using LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "y_test_bin = label_binarize(roc_data['y_test_numeric'], classes=np.unique(roc_data['y_test_numeric']))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(roc_data['y_test_numeric'], roc_data['y_pred_prob'][:, 1], pos_label=1)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yySp08R5B4M"
      },
      "source": [
        "**Since logistic regression model performance getting higer accuracy for this dataset, this indicate that the relationship between features and target variables are linear.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LxprXPey1bS"
      },
      "source": [
        "### Answer to Task 4\n",
        "\n",
        "Use three (03) different tokenization algorithms to tokenize the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEms_BUHrzb7"
      },
      "source": [
        "**1. White space tokenizer**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZTdymCMUMs5"
      },
      "source": [
        "**Reason to choose whitspace tokenizer  :**\n",
        "\n",
        "This dataset consist of text data separated from white spaces. So, using white spaces simply can tokenize the text data of the dataset. Even though this is not considering any language formats, this tokenizer is very fast due to its simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTFJDrBky1bS"
      },
      "outputs": [],
      "source": [
        "# 1. white space tokenizer\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/Purchase_Intent.csv'\n",
        "wht_tk_data = pd.read_csv(file_path)\n",
        "\n",
        "# function for white space tokenize\n",
        "def white_space_tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "#data precrossesing\n",
        "wht_preprocessed_data =dataPreprocessing(wht_tk_data,False);\n",
        "\n",
        "#classify undefined data in to yes or no classes\n",
        "wht_classified_data = classify_undefiened_data(wht_preprocessed_data);\n",
        "\n",
        "#tokenizing data by white spaces\n",
        "wht_classified_data['text'] = wht_classified_data['text'].apply(white_space_tokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy-77WrMr7M5"
      },
      "source": [
        "**2 . lemmatization**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyKHN-EyVnoy"
      },
      "source": [
        "**Reason to lemmatization   :**\n",
        "\n",
        "Lemmatization significantly reduces the vocabulary size of RNNs, CNNs, and transformers, which significantly helps in model training and speeds up prediction durations. Words that are reduced to their most basic form help models better understand various text formats, which leads to more accurate predictions. This improves generalization. Additionally, it facilitates contextual understanding by combining similar words into a single basic form, which helps to effectively capture semantic meaning.  This is more effective in task like text classification which we are currently perfomaing in this course work.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE17c4XCxE5L"
      },
      "outputs": [],
      "source": [
        "#2 stemming and lemmatizing\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/Purchase_Intent.csv'\n",
        "stem_lz_data = pd.read_csv(file_path)\n",
        "\n",
        "#preprocessing\n",
        "stem_lz_preprocessed_data = dataPreprocessing(stem_lz_data,False);\n",
        "\n",
        "#classify undefined data in to yes or no classes\n",
        "stem_lz_classified_data = classify_undefiened_data(stem_lz_preprocessed_data);\n",
        "\n",
        "#tokenizing data with using both steming and lemmatizing\n",
        "stem_lz_classified_data['text'] = stem_lz_classified_data['text'].apply(lemmatize_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBwELvg7Peb2"
      },
      "source": [
        "**3. SentencePiece**\n",
        "\n",
        "---\n",
        "\n",
        "By breaking text into subwords, SentencePiece would reduce the vocabulary size significantly. This leads to more efficient training and inference because the model can operate with a smaller set of token.SentencePiece allows a unified approach to tokenization, which results in consistency across all tokenization approaches and is necessary when using models such as transformers that rely on strict token alignment. This is achieved by a consistent input data representation approach applied throughout the model architecture.SentencePiece is also compatible with the RNN model, the CNN model. It is overall very versatile for use across different architectures, with no specific tweaks or adjustments. Also can be trained on a specific dataset that is actually being used for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQHJ6dq5lfk0"
      },
      "outputs": [],
      "source": [
        "#3. sub-word segmentation scheme\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/Purchase_Intent.csv'\n",
        "sentence_ps_data = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "def create_vocabulary():\n",
        "   with open('vocabulary.txt', 'w') as f:\n",
        "       for text in sentence_ps_data['text']:\n",
        "            f.write(text + '\\n')\n",
        "\n",
        "def train_sentence_piece_model():\n",
        "    spm.SentencePieceTrainer.train('--input=vocabulary.txt --model_prefix=spm_model --vocab_size=3000')\n",
        "\n",
        "\n",
        "\n",
        "#create a vacablury to train the sentenpiece model\n",
        "create_vocabulary()\n",
        "\n",
        "#train the sentence piece model\n",
        "train_sentence_piece_model()\n",
        "\n",
        "#load the model\n",
        "spm_model = spm.SentencePieceProcessor()\n",
        "spm_model.Load('spm_model.model')\n",
        "\n",
        "#preprocessing data\n",
        "sp_preorocessed_data =dataPreprocessing(sentence_ps_data,False);\n",
        "\n",
        "#classify undefined data in to yes or no classes\n",
        "sp_classified_data = classify_undefiened_data(sp_preorocessed_data);\n",
        "\n",
        "#tokenization using the trained model above\n",
        "sp_classfied_tokenized = sp_classified_data\n",
        "sp_classfied_tokenized['text'] = sp_classfied_tokenized['text'].apply(lambda x: spm_model.encode_as_pieces(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwFi3VhW7WmT"
      },
      "source": [
        "In above code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXcnSyeVPpIJ"
      },
      "source": [
        "**4. Create a model to check the effect for accuracy occurs from each tokenizer**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYH1WseoCyTn"
      },
      "outputs": [],
      "source": [
        "def create_model_to_check_accuracy(data_set):\n",
        "    #check if data is set of token list\n",
        "    if isinstance(data_set['text'].iloc[0],list):\n",
        "      data_set['text'] = data_set['text'].apply(lambda text : ''.join(text))\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    X = vectorizer.fit_transform(data_set['text'])\n",
        "    y = data_set['class']\n",
        "\n",
        "    #transform x  & y to numpy array\n",
        "    X = X.toarray()\n",
        "    y = np.array(y)\n",
        "\n",
        "    #split data set in to train and test data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    print(\"\\n \\n before resampling>\", Counter(y_train))\n",
        "\n",
        "    oversampler = SMOTE(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(\"\\n \\n after resampling>\", Counter(y_train_resampled))\n",
        "\n",
        "    #creating model\n",
        "    model = MultinomialNB()\n",
        "\n",
        "    #fit the train data\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "    #predict using classifier\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    #performence metrics of baseline model\n",
        "\n",
        "    if data_set['class'].dtype == 'object': #String labels\n",
        "        pos_label = 'no'\n",
        "    else:  # Numerical labels\n",
        "        pos_label = 0\n",
        "\n",
        "    #accuracy of the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    #precision of the model\n",
        "    precision_no = precision_score(y_test, y_pred,pos_label=pos_label)\n",
        "\n",
        "    #recall of the model\n",
        "    recall = recall_score(y_test, y_pred,pos_label=pos_label)\n",
        "\n",
        "    #f1 score of the model\n",
        "    f1 = f1_score(y_test, y_pred,pos_label=pos_label)\n",
        "\n",
        "    #confusion metrics for baseline model\n",
        "    cmonfusion_metrics= confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return accuracy, precision, recall, f1, cmonfusion_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCwyLjk-y1bS"
      },
      "source": [
        "Compare the performance of the three tokenizers on the intent classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACh-6iAYUGsX"
      },
      "source": [
        "**5. Display accuracy comparison of each tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCTxwKAMUUjG"
      },
      "outputs": [],
      "source": [
        "\n",
        "#logistic regression\n",
        "white_space_md_result = create_baseline_model(wht_classified_data);\n",
        "stem_lz__md_result = create_baseline_model(stem_lz_classified_data);\n",
        "sentence_ps_md_result= create_baseline_model(sp_classfied_tokenized);\n",
        "\n",
        "#extract numeric values for accuracy,precision , f1 and recall\n",
        "wht_sp_m = white_space_md_result[:4]\n",
        "stem_lz_m = stem_lz__md_result[:4]\n",
        "sp_m = sentence_ps_md_result[:4]\n",
        "\n",
        "print('\\n \\n Accuracy Comparison of Tokenizers ---- logistic regression \\n\\n')\n",
        "print('Table View:')\n",
        "display(pd.DataFrame({'white space ': wht_sp_m, ' lemmatize': stem_lz_m, 'sentence peice': sp_m}, index=['accuracy', 'precision', 'recall', 'f1_score']))\n",
        "print('\\n\\nBar Graph View: ')\n",
        "pd.DataFrame({'white space ': wht_sp_m, 'lemmatize': stem_lz_m, 'sentence peice': sp_m}, index=['accuracy', 'precision', 'recall', 'f1_score']).plot(kind = 'bar')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j7OnAmRb4fM"
      },
      "outputs": [],
      "source": [
        "#Naive bayes\n",
        "white_space_md_resultnv = create_model_to_check_accuracy(wht_classified_data);\n",
        "stem_lz__md_resultnv= create_model_to_check_accuracy(stem_lz_classified_data);\n",
        "sentence_ps_md_resultnv= create_model_to_check_accuracy(sp_classfied_tokenized);\n",
        "\n",
        "wht_sp_m_nv = white_space_md_resultnv[:4]\n",
        "stem_lz_m_nv = stem_lz__md_resultnv[:4]\n",
        "sp_m_nv= sentence_ps_md_resultnv[:4]\n",
        "\n",
        "print('\\n \\n Accuracy Comparison of Tokenizers -- Naive Bayes \\n\\n')\n",
        "print('Table View:')\n",
        "display(pd.DataFrame({'white space ': wht_sp_m_nv, ' lemmatize': stem_lz_m_nv, 'sentence peice': sp_m_nv}, index=['accuracy', 'precision', 'recall', 'f1_score']))\n",
        "print('\\n\\n \\n Bar Graph View: ')\n",
        "pd.DataFrame({'white space ': wht_sp_m_nv, 'lemmatize': stem_lz_m_nv, 'sentence peice': sp_m_nv}, index=['accuracy', 'precision', 'recall', 'f1_score']).plot(kind='bar')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB_XrEZWZv2V"
      },
      "source": [
        "**As shown in the above logistic regression model tokenizer comparison and naive bayes tokenizer comparison, logistic regression accuracy,precision and f1_core for lemmatize tokeniser has slightly higher than sentence piece tokenizer. But the recall is sligtly higher in sentence piece.**\n",
        "\n",
        " **When considering  naive bayes mode's metics of senetence piece tokenizer has higher values for accuracy,recall and f1score, which is higher than lemmatize tokenizer. Both tokenizers indicates a equal value for the precision.**\n",
        "\n",
        "\n",
        "**In both models performence metrics for white space tokenizer has lower accuracy and lower f1 score. While in logistic regression, it depicts a highest precision and in naive bayes it shows a highest recall and eqaul precision to other two tokenizers.**\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "---\n",
        "\n",
        "**1. Whitespace tokenizer -**\n",
        "\n",
        "* **Naive Bayes**: High recall, good precision, low F1 score.\n",
        "* **Logistic Regression**: Perfect precision, very low recall, and poor F1 score.\n",
        "\n",
        "  In conclusion, both models do have a tokenization problem with white space.\n",
        "\n",
        "**2. Lemmatization -**\n",
        "\n",
        "* **Naive Bayes**: Improved balanced performance across metrics.\n",
        "* **Logistic Regression**: Strong balance with precision and recall.\n",
        "\n",
        "  In conclusion, Lemmatization improves the performance of both models, making them more reliable in predicting classes either as a positive or negative class.\n",
        "\n",
        "**3. Sentence Piece -**\n",
        "\n",
        "* **Naive Bayes**: Good overall high accuracy and F1 score.\n",
        "* **Logistic regression**: Similar to lemmatization, strong and balanced performance.\n",
        "\n",
        "  In conclusion, sentence piece tokenization works for the two models, obtaining the best or almost best performing metrics. Specifically, more advanced tokenization techniques involving lemmatization and sentence piece benefit both models, with sentence piece generally leading in performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###**Final Decision**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**In below sections,since non transformer deep learning models and transformer deep learning models are used,I suggest to use the sentence piece.**\n",
        "\n",
        "**Moreover, sentence piece is capable of handling informal words while lemmatize is not capable to do so.\n",
        "Since this data set contains more informal words (since the all text in dataset are tweets), the sentence piece tokenizer will be the most suitable tokenizer.**\n",
        "\n",
        "**Lemmatization might remove some important morphological data while converting words in to there root form which is not happening in sentence piece.\n",
        "It's crucial to understand the internal structure and get the meaning of the senetence to indefity the purchasing intent.**\n",
        "\n",
        " **ex - lemmatize treats both bought and buying as same(since it converts words to the root form ex- buy) while sentence piece will capture the difference of buying and bought due to its nature of  breaking words in to subwords.(which is importaant to understand whether the user is willing to buy or has already bought).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05HKTxSUy1bS"
      },
      "source": [
        "### Answer to Task 5\n",
        "\n",
        "Explore 3 significantly different non-transformer deep learning models for building a predictive model for identifying purchase intent in a user tweet, justifying their specific architectures. Compare and contrast the results of their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_T9eN6-hTFJ"
      },
      "source": [
        "**Non-transformer models are types of artificial intelligence (AI) models that do not use the transformer architecture**.\n",
        "\n",
        "ex - RNN,CNN,LTSM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b37dJkYZg0dl"
      },
      "source": [
        "**RNN, CNN, and LSTM models can be used to examine purchase intent in user tweets. Each has a distinct architecture useful for various text analysis applications. Let's take a quick, detailed look at each model's execution,Â on how it can be used for determining purchase intent from tweets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4T8bE-1CEh7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, SimpleRNN,GlobalAveragePooling1D,Bidirectional\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.regularizers import l2,l1\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import  Precision, Recall\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "\n",
        "\n",
        "tokenized_data = sp_classfied_tokenized\n",
        "\n",
        "seed_value = 42\n",
        "tf.random.set_seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "\n",
        "# Tokenize the texts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_data['text'])\n",
        "sequences = tokenizer.texts_to_sequences(tokenized_data['text'])\n",
        "X = pad_sequences(sequences, padding='post')\n",
        "\n",
        "# Encode labels in dataset\n",
        "le = LabelEncoder()\n",
        "tokenized_data['class'] = le.fit_transform(tokenized_data['class'])\n",
        "y = tokenized_data['class'].values\n",
        "\n",
        "\n",
        "#split data set in to train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,stratify=y, random_state=42)\n",
        "\n",
        "#resampling - both undersampling and oversampling used in datset\n",
        "print(\"before resampling\", Counter(y_train))\n",
        "resampler = SMOTEENN(random_state=42)\n",
        "X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
        "print(\"after resampling\", Counter(y_train_resampled))\n",
        "\n",
        "\n",
        "\n",
        "# Convert y_train to float32\n",
        "y_train_resampled = y_train_resampled.astype('float32')\n",
        "y_test = y_test.astype('float32')\n",
        "\n",
        "# Common parameters\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = X.shape[1]\n",
        "embedding_dim = 100\n",
        "\n",
        "#gett the count of the classes in validation data\n",
        "class_counts = Counter(y_test)\n",
        "print(class_counts)\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "#method to view the history of the data\n",
        "def plot_history(history, title):\n",
        "    print('\\n\\n\\n')\n",
        "    print(\"Acuracy and Loss Comparison ---------------------------------------- \\n\")\n",
        "    plt.title(title)\n",
        "    plt.plot(history.history['val_accuracy'],label= 'Validation Accuracy')\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgHZqwILhKu1"
      },
      "source": [
        "### **1. CNN(Convolutional Neural Networks) Model**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj_eM0-TH9Lf"
      },
      "source": [
        "\n",
        "\n",
        "### **1 .Embedding Layer**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "\n",
        "  The Embedding layer is our CNN model's first layer, it is created to preprocess sequences of integers through the mapping of each word to a dense vector of fixed size (embedding_dim). It is a very important layer for text data, because it makes the transformation of sparse, categorical information into a format that neural networks can handle more efficiently.\n",
        "  \n",
        "\n",
        "*  **Why it is needed in this analysis** -\n",
        "\n",
        "  The dataset contains text data, e.g., tweets or commenting about purchasing an iPhone X. To be specific, the Embedding layer is the one that makes it possible for the model to understand the semantics of each word in the context of the text. As example, words like \"buy\" and \"purchase\" will get vectors that are closer in the embedding space, showing their close meanings.\n",
        "\n",
        "\n",
        " * **Details of used parameters** -\n",
        "\n",
        "  The vocab_size parameter is used in this layer, defines the the vocabulary size, and the embedding_dim variable indicate  the dimensionality of the embedding vectors. Input_length is the length of input sequences which ensures that the each sequence has same length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqiFE-DFHEzJ"
      },
      "source": [
        "### **2. Conv1D Layer**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "\n",
        "  Conv1D layer are purposely created for 1D convolutional operations, such as sequences (i.e. text sentences).Within these layers, filters are applied to the input sequences in order to recognize the particular aspects (such as patterns or sequences of words) that are important to the overall meaning of the sentences. we are using it for the same requirment.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*  **Why it is needed in this analysis** -\n",
        "\n",
        "  In text classification, detecting word patterns indicates the positive and negative  intent is a  important process. This layer with using its 128 filter and kernel with size of 05, is adept at capturing these patterns across different parts of the input sequences.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*  **Details of used parameters -**\n",
        "\n",
        "  As activation function, 'relu' (Rectified Linear Unit) is used in this layer. The reason to choose the 'Relu' as the activation function is it introduces non-linearity to the model & allows it to learn complex patterns in the data set.It is appropriate for deep learning models since it is also computationally efficient and helps to avoid the vanishing gradient problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQceiSKcR-H9"
      },
      "source": [
        "### **3. GlobalMaxPooling1D Layer**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "\n",
        "  The GlobalMaxPooling1D accepts the features captured by the Conv1D layer and reduces the dimentionlity of the features.This extract the most significant feature idenfied by each filter by redusing the complexity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*  **Why it is needed in this analysis** -\n",
        "\n",
        "  This layer helps to model to  identify the important features regardless of the position of the feature.It simplifies the input layer of the model and this makes traning process efficient and reduce overfitting posibility.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*  **Details of used parameters -** ---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA3-2AFAzopC"
      },
      "source": [
        "\n",
        "### **3. Dense Layers(20 units & 1 unit)**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "Dense layers are fully connected layers that let the model learn non-linear combinations of the high level features from the previous layers.The output shape remains linear in nature because it is fully connected.\n",
        "\n",
        "*  **Why it is needed in this analysis** -\n",
        "\n",
        "  The layer with 20 units acts as the decision-making layer. It starts classifying the input  based on the features that it has learned. This dense layer uses the â€˜reluâ€™ activation function to continue adding non-linearity and allowing the model to make complex decisions. It is a important addition to the model since it refines this modelâ€™s understanding before the final classification.It also containsa regularizer to avoid the overfitting.\n",
        "\n",
        "  The Dense layer with one unit is the layer that gives the final output of this model. The sigmoid activation function decide the whether the value is a range between 0 and 1 which is perfect for binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ34BbYWca1L"
      },
      "source": [
        "### **4. Dropout Layers**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "  Dropout layers with rate of 0.5 are powerful regularization method used in neural networs.Use to prevent the overfitting.\n",
        "\n",
        "*  **Why it is needed in this analysis** -\n",
        "The problem which text classification models face often is overfitting to training data, and their being not fully able to cope with noisy or varied text. Since dropout(0.5) layers deactivate a half of the neurons randomly when the model is being trained, it seems the most effective solution for avoiding the difficulties mentioned above. The model simply does not have an opportunity to become dependent on the specific words or phrases that may be found in texts, and instead a practice to look for more general patterns is developed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_zX4H0tG9nr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# CNN Model\n",
        "cnn_model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu',kernel_regularizer=l1(0.001)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dropout(0.5),\n",
        "    Dense(20, activation='relu', kernel_regularizer=l1(0.001)),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "optimiser = Adam(learning_rate=0.001)\n",
        "cnn_model.compile(optimizer=optimiser, loss='binary_crossentropy', metrics=['accuracy',Precision(name='precision', class_id=0),Recall(name='recall', class_id=0)])\n",
        "cnn_history = cnn_model.fit(X_train_resampled, y_train_resampled, epochs=500, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "print(\"\\n\\n CNN Model Summary ---------------------------------- \\n\")\n",
        "cnn_model.summary()\n",
        "\n",
        "# Compare accuracies\n",
        "cnn_metrics = cnn_model.evaluate(X_test, y_test)[1:]\n",
        "\n",
        "plot_history(cnn_history, 'CNN Model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIdNxWPugjwI"
      },
      "source": [
        "### **2. RNN(Recurrent Neural Networks)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FwFXRRPgLvf"
      },
      "source": [
        "\n",
        "\n",
        "### **1 .Embedding Layer**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "\n",
        "  The Embedding layer is our RNN model's first layer, it is created to preprocess sequences of integers through the mapping of each word to a dense vector of fixed size (embedding_dim). It is a very important layer for text data, because it makes the transformation of sparse, categorical information into a format that neural networks can handle more efficiently.\n",
        "  \n",
        "\n",
        "*  **Why it is needed in this analysis** -\n",
        "\n",
        "  The dataset contains text data, e.g., tweets or commenting about purchasing an iPhone X. To be specific, the Embedding layer is the one that makes it possible for the model to understand the semantics of each word in the context of the text. As example, words like \"buy\" and \"purchase\" will get vectors that are closer in the embedding space, showing their close meanings.\n",
        "\n",
        "\n",
        " * **Details of used parameters** -\n",
        "\n",
        "  The vocab_size parameter is used in this layer, defines the the vocabulary size, and the embedding_dim variable indicate  the dimensionality of the embedding vectors. Input_length is the length of input sequences which ensures that the each sequence has same length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKT8UYpYnSHA"
      },
      "source": [
        "### **2. RNN Layer**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "\n",
        "  This RNN layer process the word embedding sequences overtime and this layer contains of 64 unit along with ReLU as activation function. RNN is suitable for text classification since they are designed to handle sequential data.\n",
        "\n",
        "  \n",
        "*  **Why it is needed in this analysis** -\n",
        "  The sequential nature of text makes RNNs a natural choice for processing it. A SimpleRNN layer can catch the temporal dependencies between the words of the sentence, which constitute a huge part of overall meaning or sentiment understanding. While the use of ReLU (Rectified Linear Unit) does help somewhat in vanishing gradients. A reguralizer has used to minimizer the overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFSPDRcGrwaS"
      },
      "source": [
        "### **3.Dense Layers**\n",
        "\n",
        "---\n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "\n",
        "  These are responsible for further processing the features that were extracted by the RNN layer. Three fully connected layers are used, followed by 120, 80, and finally 1 unit, where the first two use the ReLU activation for getting non-linearity, and the last uses the Sigmoid for binary classification.\n",
        "\n",
        "* **Why it is needed in this analysis**\n",
        "\n",
        "  Dense layers help the model to learn complex relationships between features that were extracted by the RNN layer. Activating with ReLU assures healthy gradient flow during training. The final Dense layer, activated by sigmoid, is standard for binary classification since it provides a probability between 0 and 1, thus reflecting the chance of a positive class outcome, like purchase intent. A reguralizer has used to minimizer the overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0R5fWkAfoQr"
      },
      "source": [
        "### **4. Dropout Layers**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "  Dropout layers with rate of 0.4 are powerful regularization method used in neural networs.Use to prevent the overfitting.\n",
        "\n",
        "*  **Why it is needed in this analysis** -\n",
        "The problem which text classification models face often is overfitting to training data, and their being not fully able to cope with noisy or varied text. Since dropout(0.4) layers deactivate a half of the neurons randomly when the model is being trained, it seems the most effective solution for avoiding the difficulties mentioned above. The model simply does not have an opportunity to become dependent on the specific words or phrases that may be found in texts, and instead a practice to look for more general patterns is developed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNVXwERtG3zQ"
      },
      "outputs": [],
      "source": [
        "# RNN Model\n",
        "rnn_model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    SimpleRNN(64, activation='relu', return_sequences=False,kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.4),\n",
        "    Dense(120, activation='relu', kernel_regularizer=l1(0.001)),\n",
        "    Dropout(0.4),\n",
        "    Dense(80, activation='relu', kernel_regularizer=l1(0.001)),\n",
        "    Dropout(0.4),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "optimiser = Adam(learning_rate=0.001)\n",
        "rnn_model.compile(optimizer=optimiser, loss='binary_crossentropy', metrics=['accuracy',Precision(name='precision', class_id=0),Recall(name='recall', class_id=0)])\n",
        "rnn_history = rnn_model.fit(X_train_resampled, y_train_resampled, epochs=500, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "print(\"\\n\\n RNN Model Summary  ---------------------------------- \\n\")\n",
        "rnn_model.summary()\n",
        "\n",
        "\n",
        "# Compare accuracies\n",
        "rnn_metrics = rnn_model.evaluate(X_test, y_test)[1:]\n",
        "\n",
        "plot_history(rnn_history, 'RNN Model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS57J1YYgzau"
      },
      "source": [
        "### **3. LSTM(Long Short-Term Memory)**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp04p2nlgdcr"
      },
      "source": [
        "\n",
        "\n",
        "### **1 .Embedding Layer**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "\n",
        "  The Embedding layer is our LSTM model's first layer, it is created to preprocess sequences of integers through the mapping of each word to a dense vector of fixed size (embedding_dim). It is a very important layer for text data, because it makes the transformation of sparse, categorical information into a format that neural networks can handle more efficiently.\n",
        "  \n",
        "\n",
        "*  **Why it is needed in this analysis** -\n",
        "\n",
        "  The dataset contains text data, e.g., tweets or commenting about purchasing an iPhone X. To be specific, the Embedding layer is the one that makes it possible for the model to understand the semantics of each word in the context of the text. As example, words like \"buy\" and \"purchase\" will get vectors that are closer in the embedding space, showing their close meanings.\n",
        "\n",
        "\n",
        " * **Details of used parameters** -\n",
        "\n",
        "  The vocab_size parameter is used in this layer, defines the the vocabulary size, and the embedding_dim variable indicate  the dimensionality of the embedding vectors. Input_length is the length of input sequences which ensures that the each sequence has same length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmjUZYjiZq6H"
      },
      "source": [
        "### **2. LSTM Layer**\n",
        "\n",
        "---\n",
        "\n",
        "* **Purpose of using this layer**- processes the word embedding sequence. It is particularly useful for capturing word contexts because it has been designed to handle sequential data and may discover dependencies that were previously hundreds of steps away. With 64 units and the tanh activation function, this layer outputs sequencesâ€”that is, the entire hidden state sequence, not just the last one.\n",
        "\n",
        "*  **Why it is needed in this analysis** -\n",
        "\n",
        "  The sequential nature of text makes LSTM is a good choice to process the data. LSTMS are capable of learning dependencies between words located far in a sentence, which will be crucial to predicting the overall sentiment or intent of that particular sentence. The tanh activation function, helps maintain gradient flow during training and stops it from vanishing. L2 regularization ensures prevention of overfitting by penalizing large weights.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YSdw3iHd1L2"
      },
      "source": [
        "### **3. GlobalMaxPooling1D Layer**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "It reduces the dimensionality of output from the LSTM layer to a fixed size by taking the maximum over the time dimension for each feature.\n",
        "\n",
        "* **Why it is needed in this analysis** -\n",
        "GlobalMaxPooling1D contributes to the reduction in computational complexity and a decrease in the number of parameters. At the same time, it captures the major features of the LSTM output. This process is very helpful, especially in classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CTDBxI7X3RR"
      },
      "source": [
        "### **4.Dense Layers**\n",
        "\n",
        "---\n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "\n",
        " Fully connected layers for further processing of the features extracted by LSTM and pooling layers.\n",
        " In this model 2 dense layers are included.One with 100 units and the other with 1 unit. ReLU activation functione applied non-linearity from the first layer,while the second uses sigmoid to do binary classification\n",
        "\n",
        "* **Why it is needed in this analysis**\n",
        "\n",
        "  Dense layers help the model to learn complex relationships between features that were extracted by the RNN layer. Activating with ReLU assures healthy gradient flow during training. The final Dense layer, activated by sigmoid, is standard for binary classification since it provides a probability between 0 and 1, thus reflecting the chance of a positive class outcome, like purchase intent. A reguralizer has used to minimizer the overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I-Ja6isfxVB"
      },
      "source": [
        "### **4. Dropout Layers**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* **Purpose of using this layer**-\n",
        "  Dropout layers with rate of 0.4 are powerful regularization method used in neural networs.Use to prevent the overfitting.\n",
        "\n",
        "*  **Why it is needed in this analysis** -\n",
        "The problem which text classification models face often is overfitting to training data, and their being not fully able to cope with noisy or varied text. Since dropout(0.4) layers deactivate a half of the neurons randomly when the model is being trained, it seems the most effective solution for avoiding the difficulties mentioned above. The model simply does not have an opportunity to become dependent on the specific words or phrases that may be found in texts, and instead a practice to look for more general patterns is developed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEcOZGvDG4rF"
      },
      "outputs": [],
      "source": [
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    LSTM(units = 64, activation='tanh',return_sequences=True,kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.4),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(100, activation='relu', kernel_regularizer=l1(0.001)),\n",
        "    Dropout(0.4),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "lstm_optimiser = Adam(learning_rate=0.001)\n",
        "lstm_model.compile(optimizer=lstm_optimiser, loss='binary_crossentropy', metrics=['accuracy',Precision(name='precision', class_id=0),Recall(name='recall', class_id=0) ])\n",
        "lstm_history = lstm_model.fit(X_train_resampled, y_train_resampled, epochs=500, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "print(\"\\n\\n LSTM Model Summary  ---------------------------------- \\n\")\n",
        "lstm_model.summary()\n",
        "\n",
        "# Compare accuracies\n",
        "lstm_metrics = lstm_model.evaluate(X_test, y_test)[1:]\n",
        "\n",
        "plot_history(lstm_history, 'LSTM Model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTvUbbjHy1bS"
      },
      "source": [
        "Compare the overal results of the non-transformer deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT0ITXrBV04x"
      },
      "source": [
        "when comparing ths loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKjjQPuWnAjf"
      },
      "outputs": [],
      "source": [
        "print(f\"CNN Accuracy: {cnn_metrics[0]}\")\n",
        "print(f\"RNN Accuracy: { rnn_metrics[0]}\")\n",
        "print(f\"LSTM Accuracy: {lstm_metrics[0]}\")\n",
        "print(f\"BASELINE Accuracy: {sp_m_nv[0]}\")\n",
        "\n",
        "#define a method to get the f1 score\n",
        "def cal_f1_score(metrics):\n",
        "  return (2 * metrics[1] * metrics[2]) / (metrics[1] + metrics[2])\n",
        "\n",
        "#calculate f1 score for RNN, CNN and LSTM.\n",
        "cnn_metrics.append(cal_f1_score(cnn_metrics))\n",
        "rnn_metrics.append(cal_f1_score(rnn_metrics))\n",
        "lstm_metrics.append(cal_f1_score(lstm_metrics))\n",
        "\n",
        "print(\"\\n\\n Accuracy comparison of non transformer models with basline model- Table view\")\n",
        "display(pd.DataFrame({'CNN' :cnn_metrics,'RNN':rnn_metrics,'LSTM' : lstm_metrics, 'BASELINE': sp_m_nv},index=['accuracy', 'precision', 'recall', 'f1_score']))\n",
        "\n",
        "print(\"\\n\\n Accuracy comparison of non transformer models with basline model- Bar graph view\")\n",
        "display(pd.DataFrame({'CNN' :cnn_metrics,'RNN':rnn_metrics,'LSTM' : lstm_metrics, 'BASELINE': sp_m_nv}, index=['accuracy', 'precision', 'recall', 'f1_score']).plot(kind='bar' ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDBrBYsjLOSx"
      },
      "source": [
        "In above models, since the data set is imbalanced, precision and  recall calculated for the class that has fewer data points, which is 'no' class. The main reason to this action is the class 'yes' has a higher representation in data and model might predict that most.\n",
        "\n",
        "Considering above comparing only accuracy, might be misleading.\n",
        "Therefore in below section and above chart all metrics, accuracy, precision for minority class, recall for minority class and f1 Score is compared.\n",
        "\n",
        "### **Accuracy:**\n",
        "* **Baseline:** Accuracy value is the maximum for this one.\n",
        "* **RNN:** lower than both LSTM & Baseline.\n",
        "* **LSTM:** lower than Baseline.\n",
        "* **CNN:** Lowest; it might struggle much more with the this task.\n",
        "\n",
        "### **Precision:**\n",
        "* **Baseline:** the highest\n",
        "among the four.\n",
        "\n",
        "* **RNN**: Lower than Baseline & LSTM but higher than  CNN.\n",
        "* **LSTM**: lower than Baseline and higher than CNN and RNN\n",
        "\n",
        "* **CNN**: Low in this case.\n",
        "\n",
        " Precision measures the accuracy of the positive predictions. Highest for Baseline means that it makes the least number of errors of false positive. CNN has the lowest, which means that it makes more false positive errors compared to the others.\n",
        "\n",
        "### **Recall:**\n",
        "* **Baseline** : has the highest recall.\n",
        "* **LSTM** : lower than the Basline.\n",
        "* **RNN** : has a lower rate than the LSTM.\n",
        "* **CNN** :has the lowest recall.\n",
        "\n",
        "  Recall measures the sensitivity.of the model(true positive rate). Baseline is strong when is comes to idenfying true positive in the data set. Since CNN has the lowest, it depict a poor ability to identiy the the true positives.\n",
        "\n",
        "### **F1 Score**:\n",
        "* **Baseline**: Highest\n",
        "* **RNN**: Lower than Baseline & LSTM but higher than CNN.\n",
        "* **LSTM**: lower than basline but higher than RNN and CNN.\n",
        "* **CNN**: Lowest\n",
        "\n",
        "  The F1 score is the mean of precision and recall. Baseline model has the highest F1 score here implies good balance between precision and recall. The lowest of them is the case of CNN, which shows it does poorly in balancing both metrics.\n",
        "\n",
        "### **Overall Performance**:\n",
        "* **Baseline Model**: This models has the highest Accuracy, Precision,recall and F1-score, meaning that it performs very effectively in relation to the given task.\n",
        "\n",
        "* **LSTM Model**: Model is second model strong when it comes to all the metrics.\n",
        "Which depict that model is good at understanding the task.\n",
        "\n",
        "* **RNN Model**: The RNN model consistently performs but does not outperform the Baseline and LSTM over any specific metric. Since its loss is high need further improvements.\n",
        "\n",
        "* **CNN Model**: Performed the worst over all the metrics, meaning that it might just not be suitable for this classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7mg9zVVy1bT"
      },
      "source": [
        "### Answer to Task 6\n",
        "\n",
        "Explore encoder-only transformer models for building a predictive model for identifying purchase intent in a user tweet and compare its performance with the models developed for Task 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwDnYouyPYCZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate>=0.21.0 -U\n",
        "!pip show accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DyoR_awuplX"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,RobertaForSequenceClassification\n",
        "from datasets import Dataset, ClassLabel\n",
        "import numpy as np\n",
        "\n",
        "tokenized_data_transform = sp_preorocessed_data\n",
        "\n",
        "tokenized_texts = tokenized_data_transform['text'].tolist()\n",
        "labels = tokenized_data_transform['class'].tolist()\n",
        "\n",
        "\n",
        "#loading the tonkinizers of pretranined models.\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "#tokenized text using each model's tokenizer\n",
        "bert_tokenized_texts = bert_tokenizer(tokenized_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "roberta_tokenized_texts = roberta_tokenizer(tokenized_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "#assigning the classes and encode them by mapping them t numerical values\n",
        "class_names = ['no', 'yes']\n",
        "label_encoder = ClassLabel(num_classes=len(class_names), names=class_names, id=[0, 1])\n",
        "\n",
        "# Define compute_metrics function for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    precision, recall,f1_score, _  = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    return {\n",
        "        'accuracy': (predictions == labels).mean(),\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score,\n",
        "        }\n",
        "\n",
        "#metho implementation for initialize the trainer\n",
        "def initTrainer(pretraned_model,train_dataset,test_dataset):\n",
        "   return Trainer(\n",
        "    model=pretraned_model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "#get the data set from Dataset\n",
        "def get_dataset(tokenized_texts, labels):\n",
        "    return Dataset.from_dict({\n",
        "      'input_ids': tokenized_texts['input_ids'],\n",
        "      'attention_mask': tokenized_texts['attention_mask'],\n",
        "      'labels': label_encoder.str2int(labels)\n",
        "    })\n",
        "\n",
        "#split data set into train and test data\n",
        "def split_dataset(dataset, test_size=0.2):\n",
        "    return dataset.train_test_split(test_size=test_size)\n",
        "\n",
        "# Define training arguments for both roberta and bert\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=6,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    optim='adamw_torch',\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "#get the data set created form Dataset for both models\n",
        "bert_data_set = get_dataset(bert_tokenized_texts, labels)\n",
        "roberta_data_set = get_dataset(roberta_tokenized_texts, labels)\n",
        "\n",
        "#spliting data set in to traning and testing data\n",
        "bert_split_data = split_dataset(bert_data_set)\n",
        "roberta_split_data = split_dataset(roberta_data_set)\n",
        "\n",
        "#assigning traning and testing data for roberta\n",
        "bert_train_dataset = bert_split_data['train']\n",
        "bert_test_dataset = bert_split_data['test']\n",
        "\n",
        "#assigning traning and testing data for roberta\n",
        "roberta_train_dataset = roberta_split_data['train']\n",
        "roberta_test_dataset = roberta_split_data['test']\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_bert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUcZpKJYx12q"
      },
      "outputs": [],
      "source": [
        "#Bert - model initializing, training and evaluating\n",
        "trainer_bert = initTrainer(model_bert,bert_train_dataset,bert_test_dataset)\n",
        "trainer_bert.train()\n",
        "#getting results for bert and view\n",
        "bert_results = trainer_bert.evaluate()\n",
        "print(\"\\n\\n DistilBERT Results:\" ,bert_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffkjBEPPuhO5"
      },
      "outputs": [],
      "source": [
        "#Roberta -model initializing , training and evaluating\n",
        "trainer_roberta = initTrainer(model_roberta,roberta_train_dataset,roberta_test_dataset)\n",
        "trainer_roberta.train()\n",
        "roberta_results = trainer_roberta.evaluate()\n",
        "#gettng results for roberta and view\n",
        "print(\"\\n\\n Roberta Results:\" ,roberta_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgSPhPAQtSd_"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f\"CNN Accuracy: {cnn_metrics[0]}\")\n",
        "print(f\"RNN Accuracy: { rnn_metrics[0]}\")\n",
        "print(f\"LSTM Accuracy: {lstm_metrics[0]}\")\n",
        "print(f\"BASELINE Accuracy: {sp_m_nv[0]}\")\n",
        "print(f\"DistilBERT Accuracy: {bert_results['eval_accuracy']}\")\n",
        "print(f\"Roberta Accuracy: {roberta_results['eval_accuracy']}\")\n",
        "\n",
        "bert_eval_metrics = [bert_results['eval_accuracy'], bert_results['eval_precision'], bert_results['eval_recall'], bert_results['eval_f1_score']]\n",
        "roberta_eval_metrics = [roberta_results['eval_accuracy'], roberta_results['eval_precision'], roberta_results['eval_recall'], roberta_results['eval_f1_score']]\n",
        "\n",
        "\n",
        "print(\"\\n\\n Accuracy comparison of non transformer models with basline model- Table view\")\n",
        "display(pd.DataFrame({'CNN' :cnn_metrics,'RNN':rnn_metrics,'LSTM' : lstm_metrics,'BERT' : bert_eval_metrics, 'ROBERTA':roberta_eval_metrics },index=['accuracy', 'precision', 'recall', 'f1_score']))\n",
        "\n",
        "print(\"\\n\\n Accuracy comparison of non transformer models with basline model- Bar graph view\")\n",
        "display(pd.DataFrame({'CNN' :cnn_metrics,'RNN':rnn_metrics,'LSTM' : lstm_metrics,'BERT' : bert_eval_metrics, 'ROBERTA': roberta_eval_metrics}, index =['accuracy', 'precision', 'recall', 'f1_score']).plot(kind='bar', ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJbkwJEs3-gx"
      },
      "source": [
        "\n",
        "\n",
        "**As shown in above chart Bert and roberta has a higer value in each metric.**\n",
        "\n",
        "**Amoung Bert and Roberta, Bert has the highest values for each metric.**\n",
        "\n",
        "In this section, All metrics comapred since the data set is imbalanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQVU5f-sy1bT"
      },
      "source": [
        "### Answer to Task 7\n",
        "\n",
        "Use 3 existing pretrained large language models (of which at least 1 should be an open-source model), initially in zero-shot mode and then using few-shot prompting to identify purchase intent from tweets using a few examples from the dataset provided. Compare the performance of these models with those developed for Tasks 4 and 5 above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKyUEoz0O5QH"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Xi-6yYT0P3fS"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoForCausalLM,T5ForConditionalGeneration, T5Tokenizer,BartForConditionalGeneration, BartTokenizer\n",
        "import torch\n",
        "from transformers import ( AutoModelForCausalLM,AutoTokenizer,AutoModelForSeq2SeqLM,BartForConditionalGeneration,BartTokenizer)\n",
        "\n",
        "tokenized_data_decode = sp_classified_data\n",
        "\n",
        "\n",
        "def perform_zeroshot_GPT(tweet):\n",
        "    prompt = f\"The following is a tweet. Determine if the tweet shows purchase intent:\\nTweet: \\\"{tweet}\\\"\\nPurchase intent: Yes/No\"\n",
        "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\",padding=True)\n",
        "    attention_mask = (inputs['input_ids'] != gpt2_tokenizer.pad_token_id).long()\n",
        "    outputs = gpt2_model.generate(inputs.input_ids, max_length=inputs.input_ids.shape[1] + 10,attention_mask=attention_mask)\n",
        "    result = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return \"Yes\" if \"yes\" in result else \"No\"\n",
        "\n",
        "\n",
        "def perform_fewshot_GPT(tweet):\n",
        "  prompt = examples + f\"The following is a tweet. Determine if the tweet shows purchase intent:\\nTweet: \\\"{tweet}\\\"\\nPurchase intent: Yes/No\"\n",
        "  pad_token_id = gpt2_tokenizer.pad_token_id\n",
        "  inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\",padding='max_length',truncation=True)\n",
        "  attention_mask = (inputs['input_ids'] != gpt2_tokenizer.pad_token_id).long()\n",
        "  outputs = gpt2_model.generate(inputs.input_ids, max_length=inputs.input_ids.shape[1] + 10,attention_mask=attention_mask)\n",
        "  result = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  return \"Yes\" if \"yes\" in result else \"No\"\n",
        "\n",
        "def calculate_accuracy(model_fn, model_name):\n",
        "    correct_predictions = 0\n",
        "    tokenized_data = tokenized_data_decode[:50]\n",
        "    total_predictions = len(tokenized_data_decode)\n",
        "\n",
        "    for index, row in tokenized_data_decode.iterrows():\n",
        "        tweet = row['text']\n",
        "        actual_label = row['class']\n",
        "        predicted_label = model_fn(tweet)\n",
        "        if predicted_label.lower() == actual_label:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Accuracy for {model_name}: {accuracy:.2f}\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "examples = f\"\"\"\n",
        "Tweet :{tokenized_data_decode['text'][20]}\n",
        "Purchase intent: {tokenized_data_decode['class'][20]}\n",
        "\n",
        "Tweet :{tokenized_data_decode['text'][25]}\n",
        "Purchase intent: {tokenized_data_decode['class'][25]}\n",
        "\n",
        "Tweet :{tokenized_data_decode['text'][30]}\n",
        "Purchase intent: {tokenized_data_decode['class'][30]}\n",
        "\n",
        "Tweet :{tokenized_data_decode['text'][35]}\n",
        "Purchase intent: {tokenized_data_decode['class'][35]}\n",
        "\n",
        "Tweet :{tokenized_data_decode['text'][40]}\n",
        "Purchase intent: {tokenized_data_decode['class'][40]}\n",
        "\n",
        "Tweet :{tokenized_data_decode['text'][45]}\n",
        "Purchase intent: {tokenized_data_decode['class'][45]}\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAQhjloTwtuj"
      },
      "outputs": [],
      "source": [
        "#GPT Model\n",
        "\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "\n",
        "\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "gpt2_zeroshot = calculate_accuracy(perform_zeroshot_GPT,\"GPT-2\")\n",
        "gpt2_few_result = calculate_accuracy(lambda tweet:perform_fewshot_GPT(tweet), \"GPT Few-Shot\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx507xifUAQH"
      },
      "outputs": [],
      "source": [
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
        "\n",
        "\n",
        "\n",
        "def perform_zeroshot_T5(tweet):\n",
        "    prompt = f\"The following is a tweet. Determine if the tweet shows purchase intent:\\nTweet: \\\"{tweet}\\\"\\nPurchase intent: Yes/No\"\n",
        "    inputs = t5_tokenizer(prompt, return_tensors=\"pt\",truncation=True)\n",
        "    attention_mask = (inputs['input_ids'] != t5_tokenizer.pad_token_id).long()\n",
        "    outputs = t5_model.generate(inputs.input_ids, max_length=inputs.input_ids.shape[1] + 10,attention_mask=attention_mask)\n",
        "    result = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return \"Yes\" if \"yes\" in result else \"No\"\n",
        "\n",
        "\n",
        "def perform_fewshot_T5(tweet):\n",
        "  prompt = \"look in to these examples:\"+ examples + f\"\\nThe following is a tweet. Determine if the tweet shows purchase intent:\\nTweet: \\\"{tweet}\\\"\\nPurchase intent: Yes/No\"\n",
        "  inputs = t5_tokenizer(prompt, return_tensors=\"pt\",padding='max_length')\n",
        "  attention_mask = (inputs['input_ids'] != t5_tokenizer.pad_token_id).long()\n",
        "  outputs = t5_model.generate(inputs.input_ids, max_length=inputs.input_ids.shape[1] + 10,attention_mask=attention_mask)\n",
        "  result = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  return \"Yes\" if \"yes\" in result else \"No\"\n",
        "\n",
        "  # T5 Model\n",
        "t5_result_zero_shot = calculate_accuracy(perform_zeroshot_T5,\"T5-Zero-Shot\")\n",
        "t5_result_few_shot  = calculate_accuracy(lambda tweet:perform_fewshot_T5(tweet), \"T5 Few-Shot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4VVszoCIrLJQ"
      },
      "outputs": [],
      "source": [
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "def perform_zeroshot_BART(tweet):\n",
        "  prompt = f\"The following is a tweet. Determine if the tweet shows purchase intent:\\nTweet: \\\"{tweet}\\\"\\nPurchase intent: Yes/No\"\n",
        "  inputs = bart_tokenizer(prompt, return_tensors=\"pt\")\n",
        "  outputs = bart_model.generate(inputs.input_ids, max_length=inputs.input_ids.shape[1] + 10)\n",
        "  result = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  return \"Yes\" if \"yes\" in result else \"No\"\n",
        "\n",
        "\n",
        "def perform_fewshot_BART(tweet):\n",
        "  prompt =  \"look in to these examples:\"+ examples + f\"\\nThe following is a tweet. Determine if the tweet shows purchase intent:\\nTweet: \\\"{tweet}\\\"\\nPurchase intent: Yes/No\"\n",
        "  inputs = bart_tokenizer(prompt, return_tensors=\"pt\",padding='max_length')\n",
        "  outputs = bart_model.generate(inputs.input_ids, max_length=inputs.input_ids.shape[1] + 10)\n",
        "  result = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  return \"Yes\" if \"yes\" in result else \"No\"\n",
        "\n",
        "\n",
        "# BART Model\n",
        "bart_zero_shot = calculate_accuracy(perform_zeroshot_BART,\"BART-Zero-Shot\")\n",
        "bart_result_few_shot = calculate_accuracy(lambda tweet:perform_fewshot_BART(tweet), \"BART Few-Shot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naNHryM8uAAo"
      },
      "outputs": [],
      "source": [
        "#get results from task 5 and 6 models for above test set\n",
        "# Tokenize the texts\n",
        "tokenizer = Tokenizer()\n",
        "data = tokenized_data_decode[:50]\n",
        "tokenizer.fit_on_texts(data['text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['text'])\n",
        "X_data = pad_sequences(sequences, padding='post')\n",
        "\n",
        "# Encode labels in dataset\n",
        "le = LabelEncoder()\n",
        "data['class'] = le.fit_transform(tokenized_data['class'])\n",
        "y_data = data['class'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blwLcEjXvp_M"
      },
      "outputs": [],
      "source": [
        "cnn_metrics_new = cnn_model.evaluate(X_data, y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eld4FxCuvCIB"
      },
      "outputs": [],
      "source": [
        "rnn_metrics_new = rnn_model.evaluate(X_data, y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJpqCs5DvdGj"
      },
      "outputs": [],
      "source": [
        "lstm_metrics_new = lstm_model.evaluate(X_data, y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fpBIr_Xu8eK"
      },
      "outputs": [],
      "source": [
        "#Bert - model initializing, training and evaluating\n",
        "bert_results_new = trainer_bert.evaluate(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iojLOZ4su9HC"
      },
      "outputs": [],
      "source": [
        "\n",
        "roberta_results_new = trainer_roberta.evaluate(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aRq6vKEoZ6B"
      },
      "outputs": [],
      "source": [
        "print(f\"CNN Accuracy: {cnn_metrics_new[1]}\")\n",
        "print(f\"RNN Accuracy: {rnn_metrics_new[1]}\")\n",
        "print(f\"LSTM Accuracy: {lstm_metrics_new[1]}\")\n",
        "print(f\"BASELINE Accuracy: {sp_m_nv[0]}\")\n",
        "print(f\"DistilBERT Accuracy: {bert_results_new['eval_accuracy']}\")\n",
        "print(f\"Roberta Accuracy: {roberta_results_new['eval_accuracy']}\")\n",
        "print(f\"GPT-2 Accuracy : {gpt2_few_result}\")\n",
        "print(f\"T5 Accuracy: {t5_result_few_shot}\")\n",
        "print(f\"BART Accuracy: {bart_result_few_shot}\")\n",
        "\n",
        "print(\"\\n\\n Accuracy comparison of non transformer models and transformer model- Table view\")\n",
        "display(pd.DataFrame({\"Model\":['CNN','RNN','LSTM','BERT', 'ROBERTA','GPT-2','T5','BART'], 'Accuracies':[cnn_metrics[1], rnn_metrics[1],lstm_metrics[1],bert_results['eval_accuracy'],roberta_results['eval_accuracy'],gpt2_few_result,t5_result_few_shot,bart_result_few_shot]}))\n",
        "\n",
        "print(\"\\n\\n Accuracy comparison of non transformer models and transformer model- Bar graph view\")\n",
        "display(pd.DataFrame({\"Model\":['CNN','RNN','LSTM','BERT', 'ROBERTA','GPT-2','T5','BART'], 'Accuracies':[cnn_metrics[1], rnn_metrics[1],lstm_metrics[1],bert_results['eval_accuracy'],roberta_results['eval_accuracy'],gpt2_few_result,t5_result_few_shot,bart_result_few_shot]}, lstm_metrics = lstm_model.evaluate(X_test, y_test)\n",
        ").plot(x='Model',y='Accuracies',kind='bar'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}